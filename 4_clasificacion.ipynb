{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeBvifrnr3GY"
   },
   "source": [
    "# Clasificación de documentos (email spam o no spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/Desarrollo/estudio/Escuela de Data Science/clasificacion-con-NLTK/venv/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package punkt to /home/oem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/oem/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/oem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeBvifrnr3GY"
   },
   "source": [
    "## Ejercicio de práctica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR53vedlvd1O"
   },
   "source": [
    "¿Como podrías construir un mejor clasificador de documentos?\n",
    "\n",
    "0. **Dataset más grande:** El conjunto de datos que usamos fue muy pequeño, considera usar los archivos corpus que estan ubicados en la ruta: `datasets/email/plaintext/` \n",
    "\n",
    "1. **Limpieza:** como te diste cuenta no hicimos ningun tipo de limpieza de texto en los correos electrónicos. Considera usar expresiones regulares, filtros por categorias gramaticales, etc ... . \n",
    "\n",
    "---\n",
    "\n",
    "Con base en eso construye un dataset más grande y con un tokenizado más pulido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TOw2KrtnymVT",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Descomprimir ZIP\n",
    "import zipfile\n",
    "fantasy_zip = zipfile.ZipFile('datasets/email/plaintext/corpus1.zip')\n",
    "fantasy_zip.extractall('datasets/email/plaintext')\n",
    "fantasy_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "v2ZO0aJyrTLx",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Creamos un listado de los archivos dentro del Corpus1 ham/spam\n",
    "from os import listdir\n",
    "\n",
    "path_ham = \"datasets/email/plaintext/corpus1/ham/\"\n",
    "filepaths_ham = [path_ham+f for f in listdir(path_ham) if f.endswith('.txt')]\n",
    "\n",
    "path_spam = \"datasets/email/plaintext/corpus1/spam/\"\n",
    "filepaths_spam = [path_spam+f for f in listdir(path_spam) if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos la funcion para tokenizar y leer los archivos \n",
    "\n",
    "def abrir(texto):\n",
    "    with open(texto, 'r', errors='ignore') as f2:\n",
    "        data = f2.read()\n",
    "        data = word_tokenize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la lista tokenizada del ham\n",
    "list_ham = list(map(abrir, filepaths_ham))\n",
    "# Creamos la lista tokenizada del spam\n",
    "list_spam = list(map(abrir, filepaths_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V_KmDBHwiy8"
   },
   "source": [
    "2. **Validación del modelo anterior:**  \n",
    "---\n",
    "\n",
    "una vez tengas el nuevo conjunto de datos más pulido y de mayor tamaño, considera el mismo entrenamiento con el mismo tipo de atributos del ejemplo anterior, ¿mejora el accuracy del modelo resultante?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lC72_CbxAoJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. **Construye mejores atributos**: A veces no solo se trata de las palabras más frecuentes sino de el contexto, y capturar contexto no es posible solo viendo los tokens de forma individual, ¿que tal si consideramos bi-gramas, tri-gramas ...?, ¿las secuencias de palabras podrián funcionar como mejores atributos para el modelo?. Para ver si es así,  podemos extraer n-gramas de nuestro corpus y obtener sus frecuencias de aparición con `FreqDist()`, desarrolla tu propia manera de hacerlo y entrena un modelo con esos nuevos atributos, no olvides compartir tus resultados en la sección de comentarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 85724),\n",
       " ('.', 54709),\n",
       " ('/', 42848),\n",
       " (',', 40664),\n",
       " (':', 30278),\n",
       " ('the', 25656),\n",
       " ('to', 20345),\n",
       " ('ect', 13900),\n",
       " ('and', 12829),\n",
       " ('@', 12736),\n",
       " ('for', 10508),\n",
       " ('of', 10188),\n",
       " ('a', 9820),\n",
       " ('you', 8162),\n",
       " ('in', 7717),\n",
       " (\"'\", 7542),\n",
       " ('on', 7317),\n",
       " ('hou', 7289),\n",
       " ('this', 7171),\n",
       " ('is', 7170),\n",
       " ('?', 6881),\n",
       " ('enron', 6555),\n",
       " ('i', 6391),\n",
       " (')', 6089),\n",
       " ('(', 5758),\n",
       " ('>', 5622),\n",
       " ('Subject', 5172),\n",
       " ('be', 5067),\n",
       " ('=', 4912),\n",
       " ('that', 4769),\n",
       " (';', 4708),\n",
       " ('2000', 4386),\n",
       " ('we', 4341),\n",
       " ('from', 4192),\n",
       " ('will', 4137),\n",
       " ('have', 4097),\n",
       " ('your', 4042),\n",
       " ('with', 3987),\n",
       " ('at', 3735),\n",
       " ('com', 3710),\n",
       " ('!', 3637),\n",
       " ('s', 3435),\n",
       " ('are', 3388),\n",
       " ('it', 3335),\n",
       " ('please', 3200),\n",
       " ('as', 3157),\n",
       " ('if', 3135),\n",
       " ('or', 3080),\n",
       " ('not', 3074),\n",
       " ('gas', 3034),\n",
       " ('``', 3020),\n",
       " ('_', 3009),\n",
       " ('by', 3000),\n",
       " ('3', 2922),\n",
       " ('$', 2891),\n",
       " ('subject', 2889),\n",
       " ('deal', 2827),\n",
       " ('1', 2743),\n",
       " ('me', 2572),\n",
       " ('am', 2533),\n",
       " ('meter', 2459),\n",
       " ('00', 2404),\n",
       " ('#', 2385),\n",
       " ('2', 2379),\n",
       " ('cc', 2371),\n",
       " ('pm', 2343),\n",
       " ('hpl', 2318),\n",
       " ('can', 2143),\n",
       " ('d', 2134),\n",
       " ('000', 2127),\n",
       " ('10', 2113),\n",
       " ('our', 2092),\n",
       " ('2001', 2028),\n",
       " ('any', 2000),\n",
       " ('re', 1984),\n",
       " ('e', 1976),\n",
       " ('all', 1929),\n",
       " ('daren', 1901),\n",
       " ('thanks', 1898),\n",
       " ('01', 1794),\n",
       " ('corp', 1776),\n",
       " ('|', 1739),\n",
       " ('was', 1687),\n",
       " ('has', 1653),\n",
       " ('%', 1609),\n",
       " ('&', 1604),\n",
       " ('know', 1588),\n",
       " ('0', 1586),\n",
       " ('4', 1577),\n",
       " ('*', 1574),\n",
       " ('5', 1565),\n",
       " ('an', 1511),\n",
       " ('need', 1480),\n",
       " ('11', 1440),\n",
       " ('new', 1437),\n",
       " ('t', 1403),\n",
       " ('may', 1383),\n",
       " ('no', 1380),\n",
       " ('up', 1357),\n",
       " ('mmbtu', 1349),\n",
       " ('12', 1345),\n",
       " ('do', 1338),\n",
       " ('j', 1336),\n",
       " ('should', 1308),\n",
       " ('forwarded', 1297),\n",
       " ('get', 1276),\n",
       " ('there', 1236),\n",
       " ('http', 1235),\n",
       " ('03', 1222),\n",
       " ('price', 1206),\n",
       " ('see', 1200),\n",
       " ('company', 1198),\n",
       " ('these', 1186),\n",
       " ('let', 1160),\n",
       " ('out', 1157),\n",
       " ('information', 1154),\n",
       " ('farmer', 1141),\n",
       " ('been', 1115),\n",
       " ('l', 1108),\n",
       " ('attached', 1097),\n",
       " ('7', 1092),\n",
       " ('but', 1083),\n",
       " ('would', 1078),\n",
       " ('99', 1068),\n",
       " ('so', 1050),\n",
       " ('6', 1043),\n",
       " ('02', 1040),\n",
       " ('m', 1036),\n",
       " ('xls', 1020),\n",
       " ('us', 1018),\n",
       " ('they', 1018),\n",
       " ('what', 1010),\n",
       " ('day', 1007),\n",
       " ('time', 994),\n",
       " ('my', 993),\n",
       " ('into', 981),\n",
       " ('message', 966),\n",
       " ('only', 951),\n",
       " ('9', 949),\n",
       " ('here', 945),\n",
       " ('more', 942),\n",
       " ('04', 939),\n",
       " ('one', 935),\n",
       " ('30', 935),\n",
       " ('contract', 920),\n",
       " ('20', 918),\n",
       " ('th', 906),\n",
       " ('volume', 900),\n",
       " ('8', 894),\n",
       " ('mail', 892),\n",
       " ('robert', 886),\n",
       " ('05', 884),\n",
       " ('month', 878),\n",
       " ('sitara', 861),\n",
       " ('09', 860),\n",
       " ('about', 848),\n",
       " ('p', 848),\n",
       " ('which', 843),\n",
       " ('08', 836),\n",
       " ('email', 833),\n",
       " ('nom', 832),\n",
       " ('texas', 827),\n",
       " ('deals', 808),\n",
       " ('energy', 805),\n",
       " ('their', 796),\n",
       " ('volumes', 790),\n",
       " ('questions', 787),\n",
       " ('now', 783),\n",
       " ('15', 783),\n",
       " ('sent', 775),\n",
       " ('also', 764),\n",
       " ('+', 760),\n",
       " ('just', 758),\n",
       " ('www', 756),\n",
       " ('pec', 752),\n",
       " ('change', 735),\n",
       " ('ena', 732),\n",
       " ('some', 721),\n",
       " ('when', 712),\n",
       " ('bob', 710),\n",
       " ('production', 703),\n",
       " ('flow', 697),\n",
       " ('x', 697),\n",
       " ('call', 694),\n",
       " ('file', 684),\n",
       " ('other', 676),\n",
       " ('b', 664),\n",
       " ('like', 661),\n",
       " ('net', 659),\n",
       " ('25', 652),\n",
       " ('following', 651),\n",
       " ('06', 647),\n",
       " ('c', 633),\n",
       " ('31', 624),\n",
       " ('07', 621),\n",
       " ('over', 620),\n",
       " ('21', 611),\n",
       " ('report', 604),\n",
       " ('contact', 602),\n",
       " ('o', 599),\n",
       " ('back', 599),\n",
       " ('want', 597),\n",
       " ('nomination', 585),\n",
       " ('them', 580),\n",
       " ('he', 579),\n",
       " ('daily', 578),\n",
       " ('per', 578),\n",
       " ('could', 577),\n",
       " ('ticket', 577),\n",
       " ('16', 572),\n",
       " ('below', 570),\n",
       " ('24', 568),\n",
       " ('gary', 564),\n",
       " ('make', 562),\n",
       " ('713', 560),\n",
       " ('mary', 558),\n",
       " ('original', 556),\n",
       " ('were', 550),\n",
       " ('list', 547),\n",
       " ('march', 546),\n",
       " ('business', 546),\n",
       " ('days', 542),\n",
       " (']', 539),\n",
       " ('[', 536),\n",
       " ('number', 534),\n",
       " ('july', 529),\n",
       " ('april', 528),\n",
       " ('who', 528),\n",
       " ('its', 527),\n",
       " ('don', 526),\n",
       " ('first', 522),\n",
       " ('100', 522),\n",
       " ('through', 519),\n",
       " ('system', 518),\n",
       " ('inc', 517),\n",
       " ('font', 517),\n",
       " ('today', 516),\n",
       " ('sale', 516),\n",
       " ('14', 516),\n",
       " ('free', 515),\n",
       " ('r', 512),\n",
       " ('effective', 508),\n",
       " ('forward', 507),\n",
       " ('order', 504),\n",
       " ('td', 504),\n",
       " ('28', 501),\n",
       " ('how', 498),\n",
       " ('statements', 498),\n",
       " ('plant', 488),\n",
       " ('well', 488)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separamos las palabras mas comunes\n",
    "all_words = nltk.FreqDist([w for tokenlist in list_ham+list_spam for w in tokenlist])\n",
    "top_words = all_words.most_common(250)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('-', '-'), 65612),\n",
       " (('/', 'ect'), 7313),\n",
       " (('/', 'hou'), 7278),\n",
       " (('hou', '/'), 7278),\n",
       " (('@', 'ect'), 6547),\n",
       " (('ect', '@'), 6420),\n",
       " (('Subject', ':'), 5172),\n",
       " (('.', '.'), 4350),\n",
       " (('ect', ','), 4278),\n",
       " (('>', '>'), 3810),\n",
       " (('.', 'com'), 3650),\n",
       " (('?', '?'), 3213),\n",
       " (('to', ':'), 2766),\n",
       " (('/', '2000'), 2700),\n",
       " (('subject', ':'), 2683),\n",
       " (('@', 'enron'), 2524),\n",
       " (('/', 'enron'), 2409),\n",
       " ((\"'\", 's'), 2380),\n",
       " (('cc', ':'), 2336),\n",
       " (('of', 'the'), 2317),\n",
       " (('.', 'i'), 1877),\n",
       " (('.', 'the'), 1840),\n",
       " (('=', '='), 1786),\n",
       " (('in', 'the'), 1748),\n",
       " (('re', ':'), 1646),\n",
       " (('enron', '@'), 1629),\n",
       " ((':', 're'), 1613),\n",
       " (('if', 'you'), 1587),\n",
       " (('_', '_'), 1558),\n",
       " (('for', 'the'), 1545),\n",
       " ((',', '000'), 1446),\n",
       " (('ect', 'cc'), 1388),\n",
       " (('will', 'be'), 1378),\n",
       " (('on', 'the'), 1367),\n",
       " (('pm', 'to'), 1351),\n",
       " (('.', 'thanks'), 1318),\n",
       " ((',', 'and'), 1294),\n",
       " ((':', '/'), 1271),\n",
       " (('/', '/'), 1267),\n",
       " (('from', ':'), 1260),\n",
       " (('-', 'forwarded'), 1249),\n",
       " (('forwarded', 'by'), 1248),\n",
       " (('to', 'the'), 1225),\n",
       " (('http', ':'), 1223),\n",
       " (('/', 'corp'), 1192),\n",
       " (('corp', '/'), 1191),\n",
       " (('am', 'to'), 1114),\n",
       " ((':', 'subject'), 1097),\n",
       " (('/', '00'), 1063),\n",
       " (('.', 'please'), 1062),\n",
       " (('to', 'be'), 1051),\n",
       " ((\"'\", 't'), 1043),\n",
       " ((\"'\", ';'), 1029),\n",
       " (('you', 'have'), 1028),\n",
       " ((';', \"'\"), 1025),\n",
       " (('.', 'xls'), 1020),\n",
       " (('.', 'Subject'), 992),\n",
       " (('enron', ','), 977),\n",
       " (('.', 'this'), 957),\n",
       " ((',', 'i'), 956),\n",
       " (('daren', 'j'), 923),\n",
       " (('.', 'if'), 923),\n",
       " (('.', 'we'), 908),\n",
       " (('let', 'me'), 905),\n",
       " (('enron', '.'), 902),\n",
       " (('/', '2001'), 897),\n",
       " ((',', 'the'), 886),\n",
       " (('me', 'know'), 875),\n",
       " (('/', '01'), 870),\n",
       " (('/', 'd'), 862),\n",
       " ((',', 'please'), 858),\n",
       " (('ect', 'on'), 831),\n",
       " (('!', '!'), 806),\n",
       " ((',', '2000'), 799),\n",
       " (('j', 'farmer'), 780),\n",
       " (('is', 'a'), 766),\n",
       " (('need', 'to'), 761),\n",
       " (('com', \"'\"), 760),\n",
       " (('thanks', ','), 754),\n",
       " (('e', '-'), 753),\n",
       " (('www', '.'), 747),\n",
       " (('000', '/'), 738),\n",
       " ((',', '2001'), 735),\n",
       " (('this', 'is'), 730),\n",
       " (('ect', 'subject'), 726),\n",
       " (('with', 'the'), 715),\n",
       " (('mmbtu', '/'), 715),\n",
       " (('03', '/'), 710),\n",
       " (('.', '-'), 710),\n",
       " ((')', '.'), 695),\n",
       " (('pm', '-'), 674),\n",
       " (('*', '*'), 672),\n",
       " (('-', 'from'), 670),\n",
       " (('at', 'the'), 658),\n",
       " (('.', '000'), 650),\n",
       " (('.', '00'), 643),\n",
       " (('1', '/'), 638),\n",
       " (('should', 'be'), 636),\n",
       " (('10', '/'), 632),\n",
       " ((',', 'we'), 626),\n",
       " (('i', 'have'), 624),\n",
       " (('-', 'mail'), 617),\n",
       " (('com', '/'), 609),\n",
       " (('/', '99'), 608),\n",
       " (('am', '-'), 606),\n",
       " (('10', ':'), 592),\n",
       " (('farmer', '/'), 583),\n",
       " (('the', 'following'), 583),\n",
       " ((':', '00'), 581),\n",
       " (('i', \"'\"), 580),\n",
       " (('11', '/'), 579),\n",
       " (('is', 'the'), 575),\n",
       " (('have', 'any'), 569),\n",
       " (('/', 'hpl'), 560),\n",
       " ((')', '-'), 556),\n",
       " (('it', 'is'), 549),\n",
       " (('/', 'www'), 549),\n",
       " ((',', 'but'), 546),\n",
       " (('i', 'will'), 544),\n",
       " (('you', 'can'), 543),\n",
       " (('com', ','), 541),\n",
       " (('of', 'this'), 537),\n",
       " (('05', '/'), 536),\n",
       " (('12', '/'), 532),\n",
       " (('any', 'questions'), 526),\n",
       " (('and', 'the'), 526),\n",
       " (('i', 'am'), 519),\n",
       " (('01', '/'), 517),\n",
       " (('1', '.'), 514),\n",
       " (('.', 'net'), 513),\n",
       " (('see', 'attached'), 510),\n",
       " (('we', 'have'), 509),\n",
       " (('|', '|'), 499),\n",
       " (('don', \"'\"), 497),\n",
       " (('.', 'it'), 496),\n",
       " (('attached', 'file'), 495),\n",
       " (('message', '-'), 490),\n",
       " (('04', '/'), 490),\n",
       " (('we', 'are'), 486),\n",
       " (('xls', 'Subject'), 482),\n",
       " (('06', '/'), 482),\n",
       " (('0', '.'), 480),\n",
       " (('corp', '.'), 475),\n",
       " (('(', 'see'), 474),\n",
       " (('file', ':'), 472),\n",
       " (('the', 'deal'), 466),\n",
       " (('meter', '#'), 464),\n",
       " (('=', '3'), 464),\n",
       " (('2', '/'), 460),\n",
       " (('please', 'let'), 458),\n",
       " (('3', '/'), 456),\n",
       " (('02', '/'), 454),\n",
       " (('know', 'if'), 448),\n",
       " (('original', 'message'), 446),\n",
       " (('3', '-'), 445),\n",
       " (('has', 'been'), 445),\n",
       " (('-', 'original'), 444),\n",
       " (('.', 'you'), 442),\n",
       " (('sent', ':'), 441),\n",
       " (('11', ':'), 441),\n",
       " (('from', 'the'), 435),\n",
       " (('2', '.'), 432),\n",
       " ((':', 'hpl'), 426),\n",
       " (('fw', ':'), 426),\n",
       " ((':', 'fw'), 425),\n",
       " (('inc', '.'), 424),\n",
       " (('000', 'mmbtu'), 423),\n",
       " ((',', 'or'), 423),\n",
       " ((':', 'meter'), 419),\n",
       " (('xls', ')'), 416),\n",
       " (('have', 'been'), 416),\n",
       " (('.', 'in'), 412),\n",
       " (('deal', '#'), 410),\n",
       " (('there', 'is'), 408),\n",
       " (('the', 'company'), 405),\n",
       " (('/', 'pec'), 395),\n",
       " (('03', ':'), 394),\n",
       " (('s', '.'), 393),\n",
       " (('have', 'a'), 392),\n",
       " (('07', '/'), 390),\n",
       " (('to', 'get'), 390),\n",
       " (('08', '/'), 390),\n",
       " (('02', ':'), 387),\n",
       " (('you', 'are'), 383),\n",
       " (('that', 'the'), 383),\n",
       " (('thank', 'you'), 380),\n",
       " (('1', ','), 379),\n",
       " ((':', 'daren'), 379),\n",
       " ((',', 'daren'), 378),\n",
       " (('nbsp', ';'), 378),\n",
       " (('is', 'not'), 375),\n",
       " (('.', 'from'), 369),\n",
       " (('713', '-'), 368),\n",
       " ((':', 'robert'), 367),\n",
       " ((':', 'http'), 366),\n",
       " (('not', 'be'), 364),\n",
       " (('the', 'gas'), 363),\n",
       " (('daren', ','), 360),\n",
       " (('pec', '@'), 357),\n",
       " (('@', 'pec'), 357),\n",
       " (('2000', '10'), 356),\n",
       " (('09', ':'), 353),\n",
       " (('for', 'your'), 353),\n",
       " (('$', '0'), 353),\n",
       " (('/', '1'), 348),\n",
       " (('want', 'to'), 348),\n",
       " (('we', 'will'), 347),\n",
       " (('.', '('), 346),\n",
       " (('thanks', '.'), 345),\n",
       " (('that', 'we'), 338),\n",
       " (('enron', 'on'), 337),\n",
       " (('4', '/'), 336),\n",
       " (('6', '/'), 336),\n",
       " (('a', '.'), 335),\n",
       " (('do', 'not'), 334),\n",
       " (('teco', 'tap'), 333),\n",
       " (('=', 'http'), 333),\n",
       " (('09', '/'), 332),\n",
       " (('.', '0'), 330),\n",
       " (('me', '.'), 329),\n",
       " ((':', '$'), 329),\n",
       " (('08', ':'), 327),\n",
       " (('north', 'america'), 325),\n",
       " (('for', 'a'), 325),\n",
       " (('3', '.'), 322),\n",
       " ((',', 'inc'), 322),\n",
       " (('.', '99'), 322),\n",
       " (('01', ':'), 320),\n",
       " (('the', 'month'), 318),\n",
       " ((\"'\", 'll'), 317),\n",
       " (('04', ':'), 315),\n",
       " (('by', 'the'), 315),\n",
       " (('enron', 'cc'), 312),\n",
       " (('12', ':'), 312),\n",
       " (('pec', ','), 310),\n",
       " ((')', ','), 309),\n",
       " (('set', 'up'), 308),\n",
       " (('/', '31'), 305),\n",
       " (('to', 'you'), 305),\n",
       " (('gas', 'daily'), 304),\n",
       " (('width', '='), 304),\n",
       " (('based', 'on'), 303),\n",
       " (('tenaska', 'iv'), 299),\n",
       " ((',', 'you'), 299),\n",
       " (('.', 'for'), 296),\n",
       " (('height', '='), 296),\n",
       " (('you', 'will'), 295),\n",
       " ((',', 'melissa'), 293),\n",
       " (('on', 'this'), 292),\n",
       " ((\"'\", 'm'), 291)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agregamos Bigramas\n",
    "bigram_text = nltk.Text([w for token in list_ham+list_spam for w in token])\n",
    "bigrams = list(nltk.bigrams(bigram_text))\n",
    "top_bigrams = (nltk.FreqDist(bigrams)).most_common(250)\n",
    "top_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def document_featuresEmail(document, top_words=top_words, top_bigrams=top_bigrams):\n",
    "    document_words = set(document)\n",
    "    bigram = set(list(nltk.bigrams(nltk.Text([token for token in document]))))\n",
    "    features = {}\n",
    "    for word, j in top_words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "\n",
    "    for bigrams, i in top_bigrams:\n",
    "        features['contains_bigram({})'.format(bigrams)] = (bigrams in bigram)\n",
    "  \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Juntamos las listas indicando si tienen palabras de las mas comunes\n",
    "import random\n",
    "fset_ham = [(document_featuresEmail(texto), 0) for texto in list_ham]\n",
    "fset_spam = [(document_featuresEmail(texto), 1) for texto in list_spam]\n",
    "fset = fset_spam + fset_ham\n",
    "random.shuffle(fset)\n",
    "len(fset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fset_train, fset_test = fset[:2000], fset[2000:]\n",
    "# Entrenamos el programa\n",
    "classifier = nltk.NaiveBayesClassifier.train(fset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(document_featuresEmail(list_ham[34]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8575031525851198\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, fset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            contains(cc) = True                0 : 1      =    131.5 : 1.0\n",
      "     contains(forwarded) = True                0 : 1      =     95.3 : 1.0\n",
      "          contains(2001) = True                0 : 1      =     86.3 : 1.0\n",
      "contains_bigram(('pm', 'to')) = True                0 : 1      =     83.8 : 1.0\n",
      "contains_bigram(('am', 'to')) = True                0 : 1      =     81.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 4_clasificacion.ipynb to python\n",
      "[NbConvertApp] Writing 4269 bytes to 4_clasificacion.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to=python 4_clasificacion.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "[Lecture_19/20]Modelos_clasificacion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}